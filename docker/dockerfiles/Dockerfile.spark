# =============================================================================
# PySpark Consumer - Architecture A
# =============================================================================
FROM python:3.11-slim

LABEL maintainer="CDC POC Team"
LABEL description="PySpark Structured Streaming Consumer"

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 \
    SPARK_HOME=/opt/spark \
    PATH="${SPARK_HOME}/bin:${PATH}"

# Set working directory
WORKDIR /app

# Install system dependencies including Java
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    wget \
    openjdk-17-jdk-headless \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Download and install Spark
ENV SPARK_VERSION=3.5.0
ENV HADOOP_VERSION=3
RUN wget -q "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && tar -xzf "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" -C /opt \
    && mv "/opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" /opt/spark \
    && rm "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz"

# Copy requirements first (for better caching)
COPY requirements/base.txt requirements/base.txt
COPY requirements/spark.txt requirements/spark.txt

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements/spark.txt

# Copy application code
COPY src/spark_consumer ./src/spark_consumer
COPY src/common ./src/common
COPY configs ./configs

# Set Python path
ENV PYTHONPATH=/app/src:/app

# Create checkpoint directory
RUN mkdir -p /checkpoints && chmod 777 /checkpoints

# Expose Spark UI port
EXPOSE 4040

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:4040 || exit 1

# Run the Spark consumer
CMD ["python", "-m", "src.spark_consumer.main"]
